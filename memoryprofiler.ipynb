{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded defs\n"
     ]
    }
   ],
   "source": [
    "# This is an exploratory notebook to examine algorithm performance. At this point,\n",
    "# I am testing memory consumption and operational runtime for a couple of Python processes.\n",
    "# The basic idea is build out a suite of testing functions to evaluate submitted code snippets, \n",
    "# as either stand-alone scripts or official Python packages/modules. \n",
    "# Ultimately, tests will include examination of content.\n",
    "#\n",
    "# The justification for this work includes:\n",
    "#   1) Accepted code must be memory conservative and \n",
    "#      function correctly (as intended).\n",
    "#   2) Different technologies will be become of interest periodically. We'll need to determine\n",
    "#      which algorithms are faster, which consume less memory, which produce fewer (lower) errors, etc.\n",
    "#      \n",
    "\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from memory_profiler import profile\n",
    "import xarray as xr\n",
    "import numpy\n",
    "import os\n",
    "import io\n",
    "import yaml\n",
    "import sys\n",
    "from pyquickhelper.loghelper import run_cmd\n",
    "from mlstatpy.nlp.completion import CompletionTrieNode\n",
    "from line_profiler import LineProfiler\n",
    "\n",
    "print ('loaded defs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting blank_memoryprofiler.py\n"
     ]
    }
   ],
   "source": [
    "%%file blank_memoryprofiler.py\n",
    "\n",
    "# memory usage background + import packages\n",
    "\n",
    "from memory_profiler import profile\n",
    "\n",
    "@profile(precision=1)\n",
    "def test_df():\n",
    "    return 'test complete'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting forloop_memoryprofiler.py\n"
     ]
    }
   ],
   "source": [
    "%%file forloop_memoryprofiler.py\n",
    "\n",
    "# memory usage merely open netcdf file\n",
    "\n",
    "from memory_profiler import profile\n",
    "import xarray as xr\n",
    "import yaml\n",
    "\n",
    "# read yaml input file\n",
    "with open('file_inputs.yml', 'r') as f:\n",
    "    inputdict = yaml.load(f)\n",
    "    q_fmt = inputdict['q_fmt']\n",
    "    chunksize = inputdict['chunksize']\n",
    "    ncfile = inputdict['ncfile']\n",
    "    rconpath = inputdict['rconpath']\n",
    "    fileread = inputdict['fileread']\n",
    "    \n",
    "    # read netcdf file using dask arrays\n",
    "    if chunksize > 0:\n",
    "        xr_chk = chunksize\n",
    "    else:\n",
    "        xr_chk = {}\n",
    "        \n",
    "    if fileread:\n",
    "        ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
    "    else:\n",
    "        ds = 0\n",
    "    \n",
    "\n",
    "@profile(precision=1)\n",
    "def test_df(input_file):\n",
    "    return 'test complete'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_df('file_inputs.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Memory use background + import memory monitor\n",
      "\n",
      "Filename: blank_memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "     8     39.1 MiB      0.0 MiB   @profile(precision=1)\n",
      "     9                             def test_df():\n",
      "    10     39.1 MiB      0.0 MiB       return 'test complete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Memory use import packages + blank for loop\n",
      "\n",
      "Filename: forloop_memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    29     74.0 MiB      0.0 MiB   @profile(precision=1)\n",
      "    30                             def test_df(input_file):\n",
      "    31     74.0 MiB      0.0 MiB       return 'test complete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Memory use for loop + netcdf lazy file read (~40mb)\n",
      "\n",
      "Filename: forloop_memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    29     80.1 MiB      0.0 MiB   @profile(precision=1)\n",
      "    30                             def test_df(input_file):\n",
      "    31     80.1 MiB      0.0 MiB       return 'test complete'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test memory consumption for different chunking sizes (table rows)\n",
    "\n",
    "\n",
    "# ** test run 0: background + memory monitor package\n",
    "\n",
    "cmd = sys.executable\n",
    "cmd += \" -m memory_profiler blank_memoryprofiler.py \"\n",
    "out, err = run_cmd(cmd, wait=True)\n",
    "print(\"\\n{}\".format('Memory use background + import memory monitor'))\n",
    "print(\"\\n{}\".format(out))\n",
    "print(\"\\n{}\".format(err))\n",
    "\n",
    "\n",
    "# ** test run 1: For loop ONLY\n",
    "\n",
    "# yaml input file\n",
    "input_data = {'chunksize': 10000,\n",
    "        'rconpath' : '/Users/twellman/erddap_data/nc/test/appended.csv',\n",
    "        'ncfile' : '/Users/twellman/erddap_data/nc/NOAA_CoralReefMonitoring_LPIPercentCover_occurrence_redo_tpw_redo_tpw.nc',\n",
    "        'q_fmt' : ['\"', '\"{}\"', \"'\"],\n",
    "        'fileread': False}\n",
    "\n",
    "with open('file_inputs.yml', 'w') as outfile:\n",
    "    yaml.dump(input_data, outfile, default_flow_style=False)\n",
    "\n",
    "cmd = sys.executable\n",
    "cmd += \" -m memory_profiler forloop_memoryprofiler.py \"\n",
    "out, err = run_cmd(cmd, wait=True)\n",
    "print(\"\\n{}\".format('Memory use import packages + blank for loop'))\n",
    "print(\"\\n{}\".format(out))\n",
    "print(\"\\n{}\".format(err))\n",
    "\n",
    "\n",
    "# ** test run 2: For loop + ~40mb netcdf file read\n",
    "\n",
    "# yaml input file\n",
    "input_data = {'chunksize': False,\n",
    "        'rconpath' : '/Users/twellman/erddap_data/nc/test/appended.csv',\n",
    "        'ncfile' : '/Users/twellman/erddap_data/nc/NOAA_CoralReefMonitoring_LPIPercentCover_occurrence_redo_tpw_redo_tpw.nc',\n",
    "         'q_fmt' : ['\"', '\"{}\"', \"'\"],\n",
    "        'fileread': True}\n",
    "\n",
    "with open('file_inputs.yml', 'w') as outfile:\n",
    "    yaml.dump(input_data, outfile, default_flow_style=False)\n",
    "\n",
    "cmd = sys.executable\n",
    "cmd += \" -m memory_profiler forloop_memoryprofiler.py \"\n",
    "out, err = run_cmd(cmd, wait=True)\n",
    "print(\"\\n{}\".format('Memory use for loop + netcdf lazy file read (~40mb)'))\n",
    "print(\"\\n{}\".format(out))\n",
    "print(\"\\n{}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting memoryprofiler.py\n"
     ]
    }
   ],
   "source": [
    "%%file memoryprofiler.py\n",
    "\n",
    "# Create python script of netcdf to csv processsing function.\n",
    "\n",
    "from netCDF4 import Dataset\n",
    "from memory_profiler import profile\n",
    "import xarray as xr\n",
    "import numpy\n",
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "    \n",
    "# Add netcdf blocks to csv\n",
    "@profile(precision=1)\n",
    "def build_df(input_file):\n",
    "    \n",
    "    # read yaml inputs\n",
    "    with open(input_file, 'r') as f:\n",
    "        inputdict = yaml.load(f)\n",
    "        q_fmt = inputdict['q_fmt']\n",
    "        chunksize = inputdict['chunksize']\n",
    "        ncfile = inputdict['ncfile']\n",
    "        rconpath = inputdict['rconpath']\n",
    "\n",
    "    # read netcdf file using dask arrays\n",
    "    if chunksize > 0:\n",
    "        xr_chk = chunksize\n",
    "    else:\n",
    "        xr_chk = {}\n",
    "    ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
    "\n",
    "    # determine data processing step - multiple chunks or complete read\n",
    "    nrows = len(ds.variables['index'])\n",
    "    if chunksize > 0:\n",
    "        ndiv = int(numpy.ceil(nrows/chunksize))\n",
    "        csize = chunksize\n",
    "    else:\n",
    "        ndiv = 1\n",
    "        csize = nrows\n",
    "    \n",
    "    # convert netcdf to csv through dataframe manipulation\n",
    "    os.remove(rconpath) \n",
    "    if ndiv == 1:\n",
    "        print('Dataset processed in one read: ')\n",
    "        with open(rconpath, 'a') as csvfile:\n",
    "            df = ds.to_dataframe()\n",
    "            df.to_csv(csvfile, header=True)\n",
    "    else:\n",
    "        print('Dataset processed in chunks: ', ndiv)\n",
    "        f = True\n",
    "        with open(rconpath, 'a') as csvfile:\n",
    "            for n in numpy.arange(0, ndiv): \n",
    "                if f:\n",
    "                    sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
    "                    df = sub.to_dataframe()\n",
    "                    df.to_csv(csvfile, header=True)\n",
    "                    f = False\n",
    "                else:\n",
    "                    sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
    "                    df = sub.to_dataframe()\n",
    "                    df.to_csv(csvfile, header=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    build_df('file_inputs.yml')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset processed in one read: \n",
      "Filename: memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    14     78.3 MiB      0.0 MiB   @profile(precision=1)\n",
      "    15                             def build_df(input_file):\n",
      "    16                                 \n",
      "    17                                 # read yaml inputs\n",
      "    18     78.3 MiB      0.0 MiB       with open(input_file, 'r') as f:\n",
      "    19     78.4 MiB      0.1 MiB           inputdict = yaml.load(f)\n",
      "    20     78.4 MiB      0.0 MiB           q_fmt = inputdict['q_fmt']\n",
      "    21     78.4 MiB      0.0 MiB           chunksize = inputdict['chunksize']\n",
      "    22     78.4 MiB      0.0 MiB           ncfile = inputdict['ncfile']\n",
      "    23     78.4 MiB      0.0 MiB           rconpath = inputdict['rconpath']\n",
      "    24                             \n",
      "    25                                 # read netcdf file using dask arrays\n",
      "    26     78.4 MiB      0.0 MiB       if chunksize > 0:\n",
      "    27                                     xr_chk = chunksize\n",
      "    28                                 else:\n",
      "    29     78.4 MiB      0.0 MiB           xr_chk = 1000000 #{}\n",
      "    30     79.7 MiB      1.3 MiB       ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
      "    31                             \n",
      "    32                                 # determine data processing step - multiple chunks or complete read\n",
      "    33     79.7 MiB      0.0 MiB       nrows = len(ds.variables['index'])\n",
      "    34     79.7 MiB      0.0 MiB       if chunksize > 0:\n",
      "    35                                     ndiv = int(numpy.ceil(nrows/chunksize))\n",
      "    36                                     csize = chunksize\n",
      "    37                                 else:\n",
      "    38     79.7 MiB      0.0 MiB           ndiv = 1\n",
      "    39     79.7 MiB      0.0 MiB           csize = nrows\n",
      "    40                                 \n",
      "    41                                 # convert netcdf to csv through dataframe manipulation\n",
      "    42     79.7 MiB      0.0 MiB       os.remove(rconpath) \n",
      "    43     79.7 MiB      0.0 MiB       if ndiv == 1:\n",
      "    44     79.7 MiB      0.0 MiB           print('Dataset processed in one read: ')\n",
      "    45     79.7 MiB      0.0 MiB           with open(rconpath, 'a') as csvfile:\n",
      "    46    185.5 MiB    105.8 MiB               df = ds.to_dataframe()\n",
      "    47    188.0 MiB      2.5 MiB               df.to_csv(csvfile, header=True)\n",
      "    48                                 else:\n",
      "    49                                     print('Dataset processed in chunks: ', ndiv)\n",
      "    50                                     f = True\n",
      "    51                                     with open(rconpath, 'a') as csvfile:\n",
      "    52                                         for n in numpy.arange(0, ndiv): \n",
      "    53                                             if f:\n",
      "    54                                                 sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    55                                                 df = sub.to_dataframe()\n",
      "    56                                                 df.to_csv(csvfile, header=True)\n",
      "    57                                                 f = False\n",
      "    58                                             else:\n",
      "    59                                                 sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    60                                                 df = sub.to_dataframe()\n",
      "    61                                                 df.to_csv(csvfile, header=False)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset processed in chunks:  6\n",
      "Filename: memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    14     78.4 MiB      0.0 MiB   @profile(precision=1)\n",
      "    15                             def build_df(input_file):\n",
      "    16                                 \n",
      "    17                                 # read yaml inputs\n",
      "    18     78.4 MiB      0.0 MiB       with open(input_file, 'r') as f:\n",
      "    19     78.5 MiB      0.0 MiB           inputdict = yaml.load(f)\n",
      "    20     78.5 MiB      0.0 MiB           q_fmt = inputdict['q_fmt']\n",
      "    21     78.5 MiB      0.0 MiB           chunksize = inputdict['chunksize']\n",
      "    22     78.5 MiB      0.0 MiB           ncfile = inputdict['ncfile']\n",
      "    23     78.5 MiB      0.0 MiB           rconpath = inputdict['rconpath']\n",
      "    24                             \n",
      "    25                                 # read netcdf file using dask arrays\n",
      "    26     78.5 MiB      0.0 MiB       if chunksize > 0:\n",
      "    27     78.5 MiB      0.0 MiB           xr_chk = chunksize\n",
      "    28                                 else:\n",
      "    29                                     xr_chk = 1000000 #{}\n",
      "    30     79.9 MiB      1.4 MiB       ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
      "    31                             \n",
      "    32                                 # determine data processing step - multiple chunks or complete read\n",
      "    33     79.9 MiB      0.0 MiB       nrows = len(ds.variables['index'])\n",
      "    34     79.9 MiB      0.0 MiB       if chunksize > 0:\n",
      "    35     79.9 MiB      0.0 MiB           ndiv = int(numpy.ceil(nrows/chunksize))\n",
      "    36     79.9 MiB      0.0 MiB           csize = chunksize\n",
      "    37                                 else:\n",
      "    38                                     ndiv = 1\n",
      "    39                                     csize = nrows\n",
      "    40                                 \n",
      "    41                                 # convert netcdf to csv through dataframe manipulation\n",
      "    42     79.9 MiB      0.0 MiB       os.remove(rconpath) \n",
      "    43     79.9 MiB      0.0 MiB       if ndiv == 1:\n",
      "    44                                     print('Dataset processed in one read: ')\n",
      "    45                                     with open(rconpath, 'a') as csvfile:\n",
      "    46                                         df = ds.to_dataframe()\n",
      "    47                                         df.to_csv(csvfile, header=True)\n",
      "    48                                 else:\n",
      "    49     79.9 MiB      0.0 MiB           print('Dataset processed in chunks: ', ndiv)\n",
      "    50     79.9 MiB      0.0 MiB           f = True\n",
      "    51     79.9 MiB      0.0 MiB           with open(rconpath, 'a') as csvfile:\n",
      "    52    116.5 MiB     36.6 MiB               for n in numpy.arange(0, ndiv): \n",
      "    53    116.5 MiB      0.0 MiB                   if f:\n",
      "    54     83.1 MiB    -33.4 MiB                       sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    55    106.5 MiB     23.3 MiB                       df = sub.to_dataframe()\n",
      "    56    109.9 MiB      3.4 MiB                       df.to_csv(csvfile, header=True)\n",
      "    57    109.9 MiB      0.0 MiB                       f = False\n",
      "    58                                             else:\n",
      "    59    115.8 MiB      5.9 MiB                       sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    60    113.7 MiB     -2.1 MiB                       df = sub.to_dataframe()\n",
      "    61    116.5 MiB      2.8 MiB                       df.to_csv(csvfile, header=False)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Dataset processed in chunks:  51\n",
      "Filename: memoryprofiler.py\n",
      "\n",
      "Line #    Mem usage    Increment   Line Contents\n",
      "================================================\n",
      "    14     78.8 MiB      0.0 MiB   @profile(precision=1)\n",
      "    15                             def build_df(input_file):\n",
      "    16                                 \n",
      "    17                                 # read yaml inputs\n",
      "    18     78.8 MiB      0.0 MiB       with open(input_file, 'r') as f:\n",
      "    19     78.9 MiB      0.0 MiB           inputdict = yaml.load(f)\n",
      "    20     78.9 MiB      0.0 MiB           q_fmt = inputdict['q_fmt']\n",
      "    21     78.9 MiB      0.0 MiB           chunksize = inputdict['chunksize']\n",
      "    22     78.9 MiB      0.0 MiB           ncfile = inputdict['ncfile']\n",
      "    23     78.9 MiB      0.0 MiB           rconpath = inputdict['rconpath']\n",
      "    24                             \n",
      "    25                                 # read netcdf file using dask arrays\n",
      "    26     78.9 MiB      0.0 MiB       if chunksize > 0:\n",
      "    27     78.9 MiB      0.0 MiB           xr_chk = chunksize\n",
      "    28                                 else:\n",
      "    29                                     xr_chk = 1000000 #{}\n",
      "    30     80.8 MiB      2.0 MiB       ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
      "    31                             \n",
      "    32                                 # determine data processing step - multiple chunks or complete read\n",
      "    33     80.8 MiB      0.0 MiB       nrows = len(ds.variables['index'])\n",
      "    34     80.8 MiB      0.0 MiB       if chunksize > 0:\n",
      "    35     80.8 MiB      0.0 MiB           ndiv = int(numpy.ceil(nrows/chunksize))\n",
      "    36     80.8 MiB      0.0 MiB           csize = chunksize\n",
      "    37                                 else:\n",
      "    38                                     ndiv = 1\n",
      "    39                                     csize = nrows\n",
      "    40                                 \n",
      "    41                                 # convert netcdf to csv through dataframe manipulation\n",
      "    42     80.8 MiB      0.0 MiB       os.remove(rconpath) \n",
      "    43     80.8 MiB      0.0 MiB       if ndiv == 1:\n",
      "    44                                     print('Dataset processed in one read: ')\n",
      "    45                                     with open(rconpath, 'a') as csvfile:\n",
      "    46                                         df = ds.to_dataframe()\n",
      "    47                                         df.to_csv(csvfile, header=True)\n",
      "    48                                 else:\n",
      "    49     80.8 MiB      0.0 MiB           print('Dataset processed in chunks: ', ndiv)\n",
      "    50     80.8 MiB      0.0 MiB           f = True\n",
      "    51     80.8 MiB      0.0 MiB           with open(rconpath, 'a') as csvfile:\n",
      "    52     91.6 MiB     10.7 MiB               for n in numpy.arange(0, ndiv): \n",
      "    53     91.5 MiB     -0.0 MiB                   if f:\n",
      "    54     84.1 MiB     -7.4 MiB                       sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    55     87.0 MiB      2.9 MiB                       df = sub.to_dataframe()\n",
      "    56     87.8 MiB      0.8 MiB                       df.to_csv(csvfile, header=True)\n",
      "    57     87.8 MiB      0.0 MiB                       f = False\n",
      "    58                                             else:\n",
      "    59     91.6 MiB      3.7 MiB                       sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    60     91.6 MiB      0.0 MiB                       df = sub.to_dataframe()\n",
      "    61     91.6 MiB      0.0 MiB                       df.to_csv(csvfile, header=False)\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create initial yaml input file, save to local folder\n",
    "input_data = {'chunksize': 0,\n",
    "        'rconpath' : '/Users/twellman/erddap_data/nc/test/appended.csv',\n",
    "        'ncfile' : '/Users/twellman/erddap_data/nc/NOAA_CoralReefMonitoring_LPIPercentCover_occurrence_redo_tpw_redo_tpw.nc',\n",
    "         'q_fmt' : ['\"', '\"{}\"', \"'\"] }\n",
    "\n",
    "# Test memory consumption for different chunking sizes (table rows)\n",
    "for c in [False, 10000, 1000]:\n",
    "    input_data['chunksize'] = c\n",
    "    with open('file_inputs.yml', 'w') as outfile:\n",
    "        yaml.dump(input_data, outfile, default_flow_style=False)\n",
    "    input_data.keys()\n",
    "    cmd = sys.executable\n",
    "    cmd += \" -m memory_profiler memoryprofiler.py \"\n",
    "    out, err = run_cmd(cmd, wait=True)\n",
    "    print(\"\\n{}\".format(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def loaded\n"
     ]
    }
   ],
   "source": [
    "# repeat function in notebook cell to run line profiler\n",
    "\n",
    "\n",
    "# Add netcdf blocks to csv\n",
    "def build_df2(input_file):\n",
    "    \n",
    "    # read yaml inputs\n",
    "    with open(input_file, 'r') as f:\n",
    "        inputdict = yaml.load(f)\n",
    "        q_fmt = inputdict['q_fmt']\n",
    "        chunksize = inputdict['chunksize']\n",
    "        ncfile = inputdict['ncfile']\n",
    "        rconpath = inputdict['rconpath']\n",
    "\n",
    "    # read netcdf file using dask arrays\n",
    "    if chunksize > 0:\n",
    "        xr_chk = chunksize\n",
    "    else:\n",
    "        xr_chk = {}\n",
    "    ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
    "\n",
    "    # determine data processing step - multiple chunks or complete read\n",
    "    nrows = len(ds.variables['index'])\n",
    "    if chunksize > 0:\n",
    "        ndiv = int(numpy.ceil(nrows/chunksize))\n",
    "        csize = chunksize\n",
    "    else:\n",
    "        ndiv = 1\n",
    "        csize = nrows\n",
    "    \n",
    "    # convert netcdf to csv through dataframe manipulation\n",
    "    os.remove(rconpath) \n",
    "    if ndiv == 1:\n",
    "        print('Dataset processed in one read: ')\n",
    "        with open(rconpath, 'a') as csvfile:\n",
    "            df = ds.to_dataframe()\n",
    "            df.to_csv(csvfile, header=True)\n",
    "    else:\n",
    "        print('Dataset processed in chunks: ', ndiv)\n",
    "        f = True\n",
    "        with open(rconpath, 'a') as csvfile:\n",
    "            for n in numpy.arange(0, ndiv): \n",
    "                if f:\n",
    "                    sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
    "                    df = sub.to_dataframe()\n",
    "                    df.to_csv(csvfile, header=True)\n",
    "                    f = False\n",
    "                else:\n",
    "                    sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
    "                    df = sub.to_dataframe()\n",
    "                    df.to_csv(csvfile, header=False)\n",
    "                    \n",
    "print('def loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed in one read: \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.25438 s\n",
      "File: <ipython-input-37-ea7267e9f5f6>\n",
      "Function: build_df2 at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                           def build_df2(input_file):\n",
      "     6                                               \n",
      "     7                                               # read yaml inputs\n",
      "     8         1          169    169.0      0.0      with open(input_file, 'r') as f:\n",
      "     9         1         6407   6407.0      0.3          inputdict = yaml.load(f)\n",
      "    10         1            2      2.0      0.0          q_fmt = inputdict['q_fmt']\n",
      "    11         1            1      1.0      0.0          chunksize = inputdict['chunksize']\n",
      "    12         1            1      1.0      0.0          ncfile = inputdict['ncfile']\n",
      "    13         1          169    169.0      0.0          rconpath = inputdict['rconpath']\n",
      "    14                                           \n",
      "    15                                               # read netcdf file using dask arrays\n",
      "    16         1            1      1.0      0.0      if chunksize > 0:\n",
      "    17         1            0      0.0      0.0          xr_chk = chunksize\n",
      "    18                                               else:\n",
      "    19                                                   xr_chk = {}\n",
      "    20         1        43803  43803.0      1.9      ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
      "    21                                           \n",
      "    22                                               # determine data processing step - multiple chunks or complete read\n",
      "    23         1           10     10.0      0.0      nrows = len(ds.variables['index'])\n",
      "    24         1            1      1.0      0.0      if chunksize > 0:\n",
      "    25         1           10     10.0      0.0          ndiv = int(numpy.ceil(nrowschunksize))\n",
      "    26         1            0      0.0      0.0          csize = chunksize\n",
      "    27                                               else:\n",
      "    28                                                   ndiv = 1\n",
      "    29                                                   csize = nrows\n",
      "    30                                               \n",
      "    31                                               # convert netcdf to csv through dataframe manipulation\n",
      "    32         1         1747   1747.0      0.1      os.remove(rconpath) \n",
      "    33         1            1      1.0      0.0      if ndiv == 1:\n",
      "    34         1          393    393.0      0.0          print('Dataset processed in one read: ')\n",
      "    35         1          224    224.0      0.0          with open(rconpath, 'a') as csvfile:\n",
      "    36         1       184142 184142.0      8.2              df = ds.to_dataframe()\n",
      "    37         1      2017294 2017294.0     89.5              df.to_csv(csvfile, header=True)\n",
      "    38                                               else:\n",
      "    39                                                   print('Dataset processed in chunks: ', ndiv)\n",
      "    40                                                   f = True\n",
      "    41                                                   with open(rconpath, 'a') as csvfile:\n",
      "    42                                                       for n in numpy.arange(0, ndiv): \n",
      "    43                                                           if f:\n",
      "    44                                                               sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    45                                                               df = sub.to_dataframe()\n",
      "    46                                                               df.to_csv(csvfile, header=True)\n",
      "    47                                                               f = False\n",
      "    48                                                           else:\n",
      "    49                                                               sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    50                                                               df = sub.to_dataframe()\n",
      "    51                                                               df.to_csv(csvfile, header=False)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create initial yaml input file, save to local folder\n",
    "input_data = {'chunksize': 100000,\n",
    "        'rconpath' : '/Users/twellman/erddap_data/nc/test/appended.csv',\n",
    "        'ncfile' : '/Users/twellman/erddap_data/nc/NOAA_CoralReefMonitoring_LPIPercentCover_occurrence_redo_tpw_redo_tpw.nc',\n",
    "         'q_fmt' : ['\"', '\"{}\"', \"'\"] }\n",
    "\n",
    "with open('file_inputs.yml', 'w') as outfile:\n",
    "    yaml.dump(input_data, outfile, default_flow_style=False)\n",
    "    \n",
    "\n",
    "prof = LineProfiler()\n",
    "prof.add_function(build_df2)\n",
    "prof.run(\"build_df2('file_inputs.yml')\")\n",
    "st = io.StringIO()\n",
    "prof.print_stats(stream=st)\n",
    "rem = os.path.normpath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "res = st.getvalue().replace(rem, \"\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset processed in one read: \n",
      "Timer unit: 1e-06 s\n",
      "\n",
      "Total time: 2.17493 s\n",
      "File: <ipython-input-37-ea7267e9f5f6>\n",
      "Function: build_df2 at line 5\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "     5                                           def build_df2(input_file):\n",
      "     6                                               \n",
      "     7                                               # read yaml inputs\n",
      "     8         1          178    178.0      0.0      with open(input_file, 'r') as f:\n",
      "     9         1         5607   5607.0      0.3          inputdict = yaml.load(f)\n",
      "    10         1            7      7.0      0.0          q_fmt = inputdict['q_fmt']\n",
      "    11         1            1      1.0      0.0          chunksize = inputdict['chunksize']\n",
      "    12         1            1      1.0      0.0          ncfile = inputdict['ncfile']\n",
      "    13         1          709    709.0      0.0          rconpath = inputdict['rconpath']\n",
      "    14                                           \n",
      "    15                                               # read netcdf file using dask arrays\n",
      "    16         1            7      7.0      0.0      if chunksize > 0:\n",
      "    17                                                   xr_chk = chunksize\n",
      "    18                                               else:\n",
      "    19         1            6      6.0      0.0          xr_chk = {}\n",
      "    20         1        57994  57994.0      2.7      ds = xr.open_dataset(ncfile, chunks = xr_chk)\n",
      "    21                                           \n",
      "    22                                               # determine data processing step - multiple chunks or complete read\n",
      "    23         1           10     10.0      0.0      nrows = len(ds.variables['index'])\n",
      "    24         1            0      0.0      0.0      if chunksize > 0:\n",
      "    25                                                   ndiv = int(numpy.ceil(nrowschunksize))\n",
      "    26                                                   csize = chunksize\n",
      "    27                                               else:\n",
      "    28         1            1      1.0      0.0          ndiv = 1\n",
      "    29         1            1      1.0      0.0          csize = nrows\n",
      "    30                                               \n",
      "    31                                               # convert netcdf to csv through dataframe manipulation\n",
      "    32         1         2577   2577.0      0.1      os.remove(rconpath) \n",
      "    33         1            5      5.0      0.0      if ndiv == 1:\n",
      "    34         1          462    462.0      0.0          print('Dataset processed in one read: ')\n",
      "    35         1          311    311.0      0.0          with open(rconpath, 'a') as csvfile:\n",
      "    36         1       286372 286372.0     13.2              df = ds.to_dataframe()\n",
      "    37         1      1820682 1820682.0     83.7              df.to_csv(csvfile, header=True)\n",
      "    38                                               else:\n",
      "    39                                                   print('Dataset processed in chunks: ', ndiv)\n",
      "    40                                                   f = True\n",
      "    41                                                   with open(rconpath, 'a') as csvfile:\n",
      "    42                                                       for n in numpy.arange(0, ndiv): \n",
      "    43                                                           if f:\n",
      "    44                                                               sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    45                                                               df = sub.to_dataframe()\n",
      "    46                                                               df.to_csv(csvfile, header=True)\n",
      "    47                                                               f = False\n",
      "    48                                                           else:\n",
      "    49                                                               sub = ds.sel(index=slice(n*csize, (n+1)*csize-1))\n",
      "    50                                                               df = sub.to_dataframe()\n",
      "    51                                                               df.to_csv(csvfile, header=False)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create initial yaml input file, save to local folder\n",
    "input_data = {'chunksize': False,\n",
    "        'rconpath' : '/Users/twellman/erddap_data/nc/test/appended.csv',\n",
    "        'ncfile' : '/Users/twellman/erddap_data/nc/NOAA_CoralReefMonitoring_LPIPercentCover_occurrence_redo_tpw_redo_tpw.nc',\n",
    "         'q_fmt' : ['\"', '\"{}\"', \"'\"] }\n",
    "\n",
    "with open('file_inputs.yml', 'w') as outfile:\n",
    "    yaml.dump(input_data, outfile, default_flow_style=False)\n",
    "    \n",
    "\n",
    "prof = LineProfiler()\n",
    "prof.add_function(build_df2)\n",
    "prof.run(\"build_df2('file_inputs.yml')\")\n",
    "st = io.StringIO()\n",
    "prof.print_stats(stream=st)\n",
    "rem = os.path.normpath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\"))\n",
    "res = st.getvalue().replace(rem, \"\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3P5]",
   "language": "python",
   "name": "conda-env-Python3P5-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
